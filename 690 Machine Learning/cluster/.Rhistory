forest.Y = regression_forest(X, Y, tune.parameters = TRUE)
Y.hat = predict(forest.Y)$predictions
forest.Y.varimp = variable_importance(forest.Y)
forest.Y.varimp
forest.Y.varimp.T
t.forest.Y.varimp
transpose(forest.Y.varimp)
t(forest.Y.varimp)
forest.W = regression_forest(X, W, tune.parameters = TRUE)
W.hat = predict(forest.W)$predictions
forest.Y = regression_forest(X, Y, tune.parameters = TRUE)
Y.hat = predict(forest.Y)$predictions
forest.Y.varimp = variable_importance(forest.Y)
t(forest.Y.varimp)
forest.W = regression_forest(X, W, tune.parameters = TRUE)
W.hat = predict(forest.W)$predictions
forest.Y = regression_forest(X, Y, tune.parameters = TRUE)
Y.hat = predict(forest.Y)$predictions
forest.Y.varimp = variable_importance(forest.Y)
t(forest.Y.varimp)
# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) > 0.2)
tau.forest = causal_forest(X[, selected.vars], Y, W,
W.hat = W.hat, Y.hat = Y.hat,
tune.parameters = TRUE)
# Check whether causal forest predictions are well calibrated.
test_calibration(tau.forest)
# Check whether causal forest predictions are well calibrated.
test_calibration(tau.forest)
X.test[,1] = seq(-2, 2, length.out = 101)
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.hat = predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X[, 3])), col = 2, lty = 1)
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.hat = predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X.test[, 3])), col = 2, lty = 1)
# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) > 0.2)
tau.forest = causal_forest(X[, selected.vars], Y, W,
W.hat = W.hat, Y.hat = Y.hat,
tune.parameters = TRUE)
selected.vars
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.hat = predict(tau.forest, X.test[, selected.vars], estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X.test[, 3])), col = 2, lty = 1)
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.hat = predict(tau.forest, X.test[, selected.vars], estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X.test[, 3])), col = 2, lty = 1)
View(tau.hat)
1 / (1 + exp(-X.test[, 3])
\
1 / (1 + exp(-X.test[, 3])
)
X.test[, 3]
# Generate new data.
n = 4000; p = 20
X = matrix(rnorm(n * p), n, p)
TAU = 1 / (1 + exp(-X[, 3]))
W = rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))
Y = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)
X.test = matrix(0, 101, p)
X.test[,3] = seq(-2, 2, length.out = 101)
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.hat = predict(tau.forest, X.test[, selected.vars], estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X.test[, 3])), col = 2, lty = 1)
1 / (1 + exp(-X.test[, 3]))
# Add confidence intervals for heterogeneous treatment effects; growing more trees is now recommended.
tau.forest = causal_forest(X, Y, W, num.trees = 4000)
tau.hat = predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)
plot(X.test[,3], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[,3], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,3],  1 / (1 + exp(-X.test[, 3])), col = 2, lty = 1)
source('~/Desktop/new/functions.R')
library(readr)
read <- function(st){ read_csv(str_c("data",st,sep='/'))}
mvbr <- drop_na(read("mvbr.csv"),c("UPC"))
library(tidyverse)
mvbr <- drop_na(read("mvbr.csv"),c("UPC"))
setwd('/Users/wzm/Desktop/new_cluster')
mvbr <- drop_na(read("mvbr.csv"),c("UPC"))
problems(...)
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(tidyverse)
library(cluster)
one_hot <- function(df, cols="auto",dropCols=T){
if(cols[1] == "auto") cols <- colnames(df)[which(sapply(df, function(x) is.factor(x) & !is.ordered(x)))]
for(col in cols){
lv <- levels(df[[col]])
for(i in lv){
st <- str_c(col,i,sep = "_")
df[[st]] <- ifelse( df[[col]]==i, 1,0)
}
}
if(dropCols == T) df[!names(df) %in% cols]
else df
}
path <- function(st){ str_c("data",st,sep='/')}
n_cluster <- 10
# load upcbr #
upcbr <- read_csv(path("upcbr.csv"))
colnames(upcbr) <- tolower(colnames(upcbr))
upcbr <- plyr::rename(upcbr[2:5], c("descrip" = "name"))
upcbr <- upcbr %>% left_join( read('brand.csv'), by = c('upc' = 'UPC') ) %>%
left_join( read('vol.csv'), by = 'upc')
upcbr[is.na(upcbr$brand),]$brand <- 'other'
upcbr[is.na(upcbr$type),]$type <- 'other'
# load mvbr #
mvbr <- drop_na(read_csv(path("mvbr.csv"),col_types = cols(SALE = col_character())),c("UPC"))
colnames(mvbr) <- tolower(colnames(mvbr))
mvbr <- plyr::rename(mvbr[-c(10,11)], c("qty" = "bundle","store" = "store_id","move" = "sold"))
# load demo #
demo <- drop_na(read_csv(path("demo.csv")),c("name")) %>%
select( mmid:shpindx ) %>%
plyr::rename(c("store" = "store_id", "name" = "store"))
demo_cls <- demo %>% select(-c(mmid,store,city,zip,lat,long,store_id,scluster,zone))
demo$cluster <- clara(demo_cls,n_cluster,correct.d=TRUE)[['clustering']]
rm(demo_cls)
# load cc #
# cc <- drop_na(read('ccount.csv'),c("date","grocery")) %>%
#       rename(c("store" = "store_id"))
object.size(mvbr)
object_size(mvbr)
pryr::object_size(mvbr)
View(demo)
# merging #
mvbr <-  left_join(mvbr, upcbr, by = "upc" ) %>%
left_join( demo, by = "store_id") %>%
mutate( sale = ifelse( is.na(sale)==FALSE, sale, "N")) %>%
left_join( read_csv(path("cc_brn.csv")), by = c("store_id", "week"))
# different sale dummy #
mvbr$sale <- factor(mvbr$sale)
mvbr <- one_hot(mvbr) %>% mutate( sale = ifelse( sale_N != 1, 1,0))
mvbr$sale_N <- NULL
# different store_id dummy # unused
mvbr$store_id<-factor(mvbr$store_id) # 89
# mvbr <- one_hot(mvbr)
festival_week <- c(7,11,15,16,23,28,59,63,67,37,68,42,75,51,81,89,95,119,120,128,103,112,
116,133,141,147,156,164,168,180,185,194,199,172,173,208,216,220,224,225,
232,246,251,260,238,268,272,276,277,284,303,312,289,298,320,324,328,329,
336,341,364,372,377,50,356,380,381,389,393)
mvbr <-  mvbr %>% mutate( fstv = ifelse(week %in% festival_week, 1, 0)) %>%
mutate( rev = sold * price)
rm(festival_week)
mvbr$brand <- factor(mvbr$brand) # 48
mvbr$type <- factor(mvbr$type) # 28
pryr::object_size(mvbr)
colnames(mvbr)
###  sum(is.na(mvbr$gini)) # All none.
###   some store do not have demographic data  ###
###  sum(is.na(mvbr$educ)) # 29082
###  unique(mvbr[is.na(mvbr$educ),]$store_id) # 136 140 133 135 141 142 143 144 146 139
###  sum(mvbr$store_id %in% mvbr[is.na(mvbr$educ),]$store_id) # 29082
mvbeer <- select(mvbr,c( store_id, week, upc, rev, sale, vol, fstv,cluster,price,sale_B,sale_S))
mvbeer <- mvbeer %>% group_by(store_id, week) %>%
mutate(highrev = ifelse(rev >= sum(rev)/n_distinct(upc) && rev>0, 1, 0) )
## quantilization ## unused
# mvbeer <- mvbeer %>%
#   mutate( ethn = as.numeric( ( cut(mvbeer$ethnic, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age9 = as.numeric( ( cut(mvbeer$age9, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age60 = as.numeric( ( cut(mvbeer$age60, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( educ = as.numeric( ( cut(mvbeer$educ, breaks =seq(0,1, by =0.05)))))
colnames(mvbr)
###  sum(is.na(mvbr$gini)) # All none.
###   some store do not have demographic data  ###
###  sum(is.na(mvbr$educ)) # 29082
###  unique(mvbr[is.na(mvbr$educ),]$store_id) # 136 140 133 135 141 142 143 144 146 139
###  sum(mvbr$store_id %in% mvbr[is.na(mvbr$educ),]$store_id) # 29082
mvbeer <- select(mvbr,c( store_id, week, upc, rev, sale, vol, fstv,cluster,price,sale_B:sale_S))
mvbeer <- mvbeer %>% group_by(store_id, week) %>%
mutate(highrev = ifelse(rev >= sum(rev)/n_distinct(upc) && rev>0, 1, 0) )
## quantilization ## unused
# mvbeer <- mvbeer %>%
#   mutate( ethn = as.numeric( ( cut(mvbeer$ethnic, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age9 = as.numeric( ( cut(mvbeer$age9, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age60 = as.numeric( ( cut(mvbeer$age60, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( educ = as.numeric( ( cut(mvbeer$educ, breaks =seq(0,1, by =0.05)))))
pryr::object_size(mvbeer)
rm(demo);rm(mvbr);rm(upcbr);rm(one_hot);rm(read)
write_csv(mvbeer,"data/mvbeer.csv")
rm(path)
rm(n_cluster)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(grf)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(grf)
library(ggplot2)
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
for(i in c('store_id','brand','type')) {mvbeer[[i]] <- as.numeric(mvbeer[[i]])}
View(mvbeer)
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
X <- as.matrix( select(mvbeer, -c(rev, sale,sale_S,sale_B,upc)))
y <- mvbeer$rev
w <- mvbeer$sale
rm(mvbeer);rm(i)
set.seed(100)
mtry <- ceiling(ncol(X)/3)
model <- causal_forest( X, y, w , mtry = mtry ,
num.trees = 200, honesty = TRUE, honesty.fraction = 0.5 )
print( model )
cat('\n')
print( average_treatment_effect(model) )
cat('\n')
print(colnames(X))
save.image("~/Desktop/cluster_200.RData")
sum(is.na(X[["highrev" ]]))
sum(is.na(X["highrev"]))
sum(X["highrev"]!=0)
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(plyr)
library(tidyverse)
one_hot <- function(df, cols="auto",dropCols=T){
if(cols[1] == "auto") cols <- colnames(df)[which(sapply(df, function(x) is.factor(x) & !is.ordered(x)))]
for(col in cols){
lv <- levels(df[[col]])
for(i in lv){
st <- str_c(col,i,sep = "_")
df[[st]] <- ifelse( df[[col]]==i, 1,0)
}
}
if(dropCols == T) df[!names(df) %in% cols]
else df
}
path <- function(st){ str_c("data",st,sep='/')}
n_cluster <- 10
# load upcbr #
upcbr <- read_csv(path("upcbr.csv"))
colnames(upcbr) <- tolower(colnames(upcbr))
upcbr <- plyr::rename(upcbr[2:5], c("descrip" = "name"))
upcbr <- upcbr %>% left_join( read('brand.csv'), by = c('upc' = 'UPC') ) %>%
left_join( read('vol.csv'), by = 'upc')
one_hot <- function(df, cols="auto",dropCols=T){
if(cols[1] == "auto") cols <- colnames(df)[which(sapply(df, function(x) is.factor(x) & !is.ordered(x)))]
for(col in cols){
lv <- levels(df[[col]])
for(i in lv){
st <- str_c(col,i,sep = "_")
df[[st]] <- ifelse( df[[col]]==i, 1,0)
}
}
if(dropCols == T) df[!names(df) %in% cols]
else df
}
path <- function(st){ str_c("data",st,sep='/')}
n_cluster <- 10
# load upcbr #
upcbr <- read_csv(path("upcbr.csv"))
colnames(upcbr) <- tolower(colnames(upcbr))
upcbr <- plyr::rename(upcbr[2:5], c("descrip" = "name"))
upcbr <- upcbr %>% left_join( read_csv(path('brand.csv')), by = c('upc' = 'UPC') ) %>%
left_join( read_csv(path('vol.csv')), by = 'upc')
upcbr[is.na(upcbr$brand),]$brand <- 'other'
upcbr[is.na(upcbr$type),]$type <- 'other'
# load mvbr #
mvbr <- drop_na(read_csv(path("mvbr.csv"),col_types = cols(SALE = col_character())),c("UPC"))
colnames(mvbr) <- tolower(colnames(mvbr))
mvbr <- plyr::rename(mvbr[-c(10,11)], c("qty" = "bundle","store" = "store_id","move" = "sold"))
# load demo #
demo <- drop_na(read_csv(path("demo.csv")),c("name")) %>%
select( mmid:shpindx ) %>%
plyr::rename(c("store" = "store_id", "name" = "store"))
demo_cls <- demo %>% select(-c(mmid,store,city,zip,lat,long,store_id,scluster,zone))
demo$cluster <- clara(demo_cls,n_cluster,correct.d=TRUE)[['clustering']]
rm(demo_cls)
# load cc #
# cc <- drop_na(read('ccount.csv'),c("date","grocery")) %>%
#       rename(c("store" = "store_id"))
# merging #
mvbr <-  left_join(mvbr, upcbr, by = "upc" ) %>%
left_join( demo, by = "store_id") %>%
mutate( sale = ifelse( is.na(sale)==FALSE, sale, "N")) %>%
left_join( read_csv(path("cc_brn.csv")), by = c("store_id", "week"))
# different sale dummy #
mvbr$sale <- factor(mvbr$sale)
mvbr <- one_hot(mvbr) %>% mutate( sale = ifelse( sale_N != 1, 1,0))
mvbr$sale_N <- NULL
# different store_id dummy # unused
mvbr$store_id<-factor(mvbr$store_id) # 89
# mvbr <- one_hot(mvbr)
festival_week <- c(7,11,15,16,23,28,59,63,67,37,68,42,75,51,81,89,95,119,120,128,103,112,
116,133,141,147,156,164,168,180,185,194,199,172,173,208,216,220,224,225,
232,246,251,260,238,268,272,276,277,284,303,312,289,298,320,324,328,329,
336,341,364,372,377,50,356,380,381,389,393)
mvbr <-  mvbr %>% mutate( fstv = ifelse(week %in% festival_week, 1, 0)) %>%
mutate( rev = sold * price)
rm(festival_week)
mvbr$brand <- factor(mvbr$brand) # 48
mvbr$type <- factor(mvbr$type) # 28
colnames(mvbr)
###  sum(is.na(mvbr$gini)) # All none.
###   some store do not have demographic data  ###
###  sum(is.na(mvbr$educ)) # 29082
###  unique(mvbr[is.na(mvbr$educ),]$store_id) # 136 140 133 135 141 142 143 144 146 139
###  sum(mvbr$store_id %in% mvbr[is.na(mvbr$educ),]$store_id) # 29082
mvbeer <- select(mvbr,c( store_id, week, upc, rev, sale, vol, fstv,cluster,price,sale_B:sale_S))
mvbeer <- mvbeer %>% group_by(store_id, week) %>%
mutate(highrev = ifelse(rev >= sum(rev)/n_distinct(upc) && rev>0, 1, 0) )
## quantilization ## unused
# mvbeer <- mvbeer %>%
#   mutate( ethn = as.numeric( ( cut(mvbeer$ethnic, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age9 = as.numeric( ( cut(mvbeer$age9, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age60 = as.numeric( ( cut(mvbeer$age60, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( educ = as.numeric( ( cut(mvbeer$educ, breaks =seq(0,1, by =0.05)))))
sum(mvbeer$highrev==1)
sum(mvbeer$highrev==0)
rm(demo);rm(mvbr);rm(upcbr);rm(one_hot);rm(path);rm(n_cluster)
write_csv(mvbeer,"data/mvbeer.csv")
start_time = Sys.time()
Sys.sleep(0.5)
end_time = Sys.time()
end_time - start_time
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(grf)
library(ggplot2)
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
X <- as.matrix( select(mvbeer, -c(rev, sale,sale_S,sale_C,sale_B,upc)))
y <- mvbeer$rev
w <- mvbeer$sale
rm(mvbeer);rm(i)
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
X <- as.matrix( select(mvbeer, -c(rev, sale,sale_S,sale_C,sale_B,upc)))
y <- mvbeer$rev
w <- mvbeer$sale
rm(mvbeer)
View(X)
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(plyr)
library(tidyverse)
one_hot <- function(df, cols="auto",dropCols=T){
if(cols[1] == "auto") cols <- colnames(df)[which(sapply(df, function(x) is.factor(x) & !is.ordered(x)))]
for(col in cols){
lv <- levels(df[[col]])
for(i in lv){
st <- str_c(col,i,sep = "_")
df[[st]] <- ifelse( df[[col]]==i, 1,0)
}
}
if(dropCols == T) df[!names(df) %in% cols]
else df
}
path <- function(st){ str_c("data",st,sep='/')}
n_cluster <- 10
# load upcbr #
upcbr <- read_csv(path("upcbr.csv"))
colnames(upcbr) <- tolower(colnames(upcbr))
upcbr <- plyr::rename(upcbr[2:5], c("descrip" = "name"))
upcbr <- upcbr %>% left_join( read_csv(path('brand.csv')), by = c('upc' = 'UPC') ) %>%
left_join( read_csv(path('vol.csv')), by = 'upc')
upcbr[is.na(upcbr$brand),]$brand <- 'other'
upcbr[is.na(upcbr$type),]$type <- 'other'
# load mvbr #
mvbr <- drop_na(read_csv(path("mvbr.csv"),col_types = cols(SALE = col_character())),c("UPC"))
colnames(mvbr) <- tolower(colnames(mvbr))
mvbr <- plyr::rename(mvbr[-c(10,11)], c("qty" = "bundle","store" = "store_id","move" = "sold"))
# load demo #
demo <- drop_na(read_csv(path("demo.csv")),c("name")) %>%
select( mmid:shpindx ) %>%
plyr::rename(c("store" = "store_id", "name" = "store"))
demo_cls <- demo %>% select(-c(mmid,store,city,zip,lat,long,store_id,scluster,zone))
demo$cluster <- clara(demo_cls,n_cluster,correct.d=TRUE)[['clustering']]
rm(demo_cls)
# load cc #
# cc <- drop_na(read('ccount.csv'),c("date","grocery")) %>%
#       rename(c("store" = "store_id"))
# merging #
mvbr <-  left_join(mvbr, upcbr, by = "upc" ) %>%
left_join( demo, by = "store_id") %>%
mutate( sale = ifelse( is.na(sale)==FALSE, sale, "N")) %>%
left_join( read_csv(path("cc_brn.csv")), by = c("store_id", "week"))
# different sale dummy #
mvbr$sale <- factor(mvbr$sale)
mvbr <- one_hot(mvbr) %>% mutate( sale = ifelse( sale_N != 1, 1,0))
mvbr$sale_N <- NULL
# different store_id dummy # unused
mvbr$store_id<-factor(mvbr$store_id) # 89
# mvbr <- one_hot(mvbr)
festival_week <- c(7,11,15,16,23,28,59,63,67,37,68,42,75,51,81,89,95,119,120,128,103,112,
116,133,141,147,156,164,168,180,185,194,199,172,173,208,216,220,224,225,
232,246,251,260,238,268,272,276,277,284,303,312,289,298,320,324,328,329,
336,341,364,372,377,50,356,380,381,389,393)
mvbr <-  mvbr %>% mutate( fstv = ifelse(week %in% festival_week, 1, 0)) %>%
mutate( rev = sold * price)
rm(festival_week)
mvbr$brand <- factor(mvbr$brand) # 48
mvbr$type <- factor(mvbr$type) # 28
colnames(mvbr)
###  sum(is.na(mvbr$gini)) # All none.
###   some store do not have demographic data  ###
###  sum(is.na(mvbr$educ)) # 29082
###  unique(mvbr[is.na(mvbr$educ),]$store_id) # 136 140 133 135 141 142 143 144 146 139
###  sum(mvbr$store_id %in% mvbr[is.na(mvbr$educ),]$store_id) # 29082
mvbeer <- select(mvbr,c( store_id, week, upc,brand, type, rev, sale, vol, fstv,cluster,price,sale_B:sale_S))
mvbeer <- mvbeer %>% group_by(store_id, week) %>%
dplyr::mutate(highrev = ifelse(rev >= sum(rev)/n_distinct(upc) && rev>0, 1, 0) )
## quantilization ## unused
# mvbeer <- mvbeer %>%
#   mutate( ethn = as.numeric( ( cut(mvbeer$ethnic, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age9 = as.numeric( ( cut(mvbeer$age9, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( age60 = as.numeric( ( cut(mvbeer$age60, breaks =seq(0,1, by =0.05))))) %>%
#   mutate( educ = as.numeric( ( cut(mvbeer$educ, breaks =seq(0,1, by =0.05)))))
rm(demo);rm(mvbr);rm(upcbr);rm(one_hot);rm(path);rm(n_cluster)
write_csv(mvbeer,"data/mvbeer.csv")
View(mvbeer)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(grf)
library(ggplot2)
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
View(mvbeer)
mvbeer$store_id <- factor(mvbeer$store_id)
for(i in c('store_id','brand','type')) {mvbeer[[i]] <- as.numeric(mvbeer[[i]])}
X <- as.matrix( select(mvbeer, -c(rev, sale,sale_S,sale_C,sale_B,upc)))
y <- mvbeer$rev
w <- mvbeer$sale
mvbeer <- drop_na(read.csv("data/mvbeer.csv"))
mvbeer$store_id <- factor(mvbeer$store_id)
for(i in c('store_id','brand','type')) {mvbeer[[i]] <- as.numeric(mvbeer[[i]])}
X <- as.matrix( select(mvbeer, -c(rev, sale,sale_S,sale_C,sale_B,upc)))
y <- mvbeer$rev
w <- mvbeer$sale
rm(mvbeer);rm(i)
View(X)
set.seed(100)
mtry <- ceiling(ncol(X)/3)
start_time <- Sys.time()
model <- causal_forest( X, y, w , mtry = mtry ,
num.trees = 200, honesty = TRUE, honesty.fraction = 0.5 )
end_time <- Sys.time()
end_time - start_time
print( model )
cat('\n')
print( average_treatment_effect(model) )
cat('\n')
print(colnames(X))
save.image("~/Downloads/cluster_9_200.RData")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(grf)
library(ggplot2)
source('functions.R')
treat <- avg_treat(X)
plot_part_avg(treat,"price",sav = "price.pdf")
plot_part_avg(treat,"week",sav = "week.pdf")
# plot_part_avg(treat,"brand")
# plot_part_avg(treat,"type")
# plot_part_avg(treat,"fstv",type="p")
# plot_part_avg(treat,"highrev",type="p")
source('functions.R')
treat <- avg_treat(X)
plot_part_avg(treat,"price",sav = "price.pdf")
plot_part_avg(treat,"week",sav = "week.pdf")
# plot_part_avg(treat,"brand")
# plot_part_avg(treat,"type")
# plot_part_avg(treat,"fstv",type="p")
# plot_part_avg(treat,"highrev",type="p")
avg_box(treat,c("week","price"),"week_price.pdf")
avg_box(treat,c("brand","type"),"brand_type.pdf")
avg_box(treat,c("fstv","highrev"),"fstv_highrev.pdf")
plot_part_avg(treat,"price",sav = "price.pdf")
plot_part_avg(treat,"week",sav = "week.pdf")
plot_part_avg(treat,"brand")
plot_part_avg(treat,"type")
# plot_part_avg(treat,"fstv",type="p")
# plot_part_avg(treat,"highrev",type="p")
plot_part_avg(treat,"price",sav = "price.pdf")
plot_part_avg(treat,"week",sav = "week.pdf")
plot_part_avg(treat,"brand",sav = "brand.pdf")
plot_part_avg(treat,"type",sav = "type.pdf")
plot_part_avg(treat,"cluster",sav = "cluster.pdf")
# plot_part_avg(treat,"fstv",type="p")
# plot_part_avg(treat,"highrev",type="p")
avg_box(treat,c("week","price"),"week_price.pdf")
avg_box(treat,c("brand","type"),"brand_type.pdf")
avg_box(treat,c("fstv","highrev"),"fstv_highrev.pdf")
avg_box(treat,c("vol","cluster"),"vol_cluster.pdf")
plot_part(X,8, sav = "partial_price.pdf")
plot_part(X,7, sav = "partial_cluster.pdf")
plot_part(X,2, sav = "partial_week.pdf")
plot_part(X,3, sav = "partial_brand.pdf")
plot_part(X,4, sav = "partial_type.pdf")
